{"cells":[{"cell_type":"code","source":["%%shell\n","jupyter nbconvert --to html /content/P1.ipynb"],"metadata":{"id":"y1-dR7FZ2pda","executionInfo":{"status":"ok","timestamp":1653039876639,"user_tz":-60,"elapsed":2366,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"2382557c-54ea-45cc-ebc3-cefa4de5ad5a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/P1.ipynb to html\n","[NbConvertApp] Writing 354920 bytes to /content/P1.html\n"]},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"gCAPWR2s3nzA"},"source":["# Data Science Portfolio - Part I (30 marks)\n","\n","In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n","\n","The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n","\n","In this exercise, you are asked to perform a number of operations to gain insights from the data."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7Pm74v1u4d6G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653039718512,"user_tz":-60,"elapsed":1122,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"d5a791f5-1f3a-4093-d610-3b33f6056e70"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["# C21102236\n","# suggested imports\n","import pandas as pd\n","from nltk.tag import pos_tag\n","import re\n","from collections import defaultdict,Counter\n","from nltk.stem import WordNetLemmatizer\n","from datetime import datetime\n","from tqdm import tqdm\n","import numpy as np\n","import os\n","tqdm.pandas()\n","from ast import literal_eval\n","# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize,sent_tokenize"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WfNsDQ253nzJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653039719383,"user_tz":-60,"elapsed":878,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"a906680e-18d9-48e6-ebec-868d642de390"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\n"]}],"source":["from urllib import request\n","import pandas as pd\n","module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\"\n","module_name = module_url.split('/')[-1]\n","print(f'Fetching {module_url}')\n","#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n","with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n","  a = f.read()\n","  outf.write(a.decode('utf-8'))\n","\n","\n","df = pd.read_csv('data_portfolio_21.csv')\n","# this fills empty cells with empty strings\n","df = df.fillna('')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"CNfbxg2X3nzK","colab":{"base_uri":"https://localhost:8080/","height":861},"executionInfo":{"status":"ok","timestamp":1653039719384,"user_tz":-60,"elapsed":10,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"45c845cf-8b15-46b2-cffb-2fcef1e321f8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       author            posted_at  num_comments  score selftext  \\\n","0  -Howitzer-  2020-08-17 20:26:04            19      1            \n","1  -Howitzer-  2020-07-06 17:01:48             1      3            \n","2  -Howitzer-  2020-09-09 02:29:02             3      1            \n","3  -Howitzer-  2020-06-23 23:02:39             2      1            \n","4  -Howitzer-  2020-08-07 04:13:53            32    622            \n","5  -Howitzer-  2020-11-17 16:33:29            30     26            \n","6  -Howitzer-  2020-08-19 17:04:16            45      1            \n","7  -Howitzer-  2020-09-04 13:38:53             2      1            \n","8  -Howitzer-  2020-09-03 20:16:52             6      1            \n","9  -Howitzer-  2020-06-30 22:13:30             7      2            \n","\n","  subr_created_at              subr_description  \\\n","0      2009-04-29  Subreddit about Donald Trump   \n","1      2009-04-29  Subreddit about Donald Trump   \n","2      2009-04-29  Subreddit about Donald Trump   \n","3      2009-04-29  Subreddit about Donald Trump   \n","4      2009-04-29  Subreddit about Donald Trump   \n","5      2009-04-29  Subreddit about Donald Trump   \n","6      2009-04-29  Subreddit about Donald Trump   \n","7      2009-04-29  Subreddit about Donald Trump   \n","8      2009-04-29  Subreddit about Donald Trump   \n","9      2009-04-29  Subreddit about Donald Trump   \n","\n","                                       subr_faved_by  subr_numb_members  \\\n","0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","5  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","6  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","7  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","8  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","9  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","\n","   subr_numb_posts    subreddit  \\\n","0           796986  donaldtrump   \n","1           796986  donaldtrump   \n","2           796986  donaldtrump   \n","3           796986  donaldtrump   \n","4           796986  donaldtrump   \n","5           796986  donaldtrump   \n","6           796986  donaldtrump   \n","7           796986  donaldtrump   \n","8           796986  donaldtrump   \n","9           796986  donaldtrump   \n","\n","                                               title  total_awards_received  \\\n","0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n","1                                Joe Biden's America                      0   \n","2  4 more years and we can erase his legacy for g...                      0   \n","3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n","4                                     LOOK HERE, FAT                      0   \n","5  Here Is The Evidence – Crowdsourcing Evidence ...                      0   \n","6                                               PJWs                      0   \n","7                                CAN'T CUCK THE TUCK                      0   \n","8  Portland Antifa is Being Housed By City Democr...                      0   \n","9  Citing State of Affairs in US, Schiff Jokes 'W...                      0   \n","\n","   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n","0          1.00            4661         2012-11-09          -0.658599  \n","1          0.67            4661         2012-11-09          -0.658599  \n","2          1.00            4661         2012-11-09          -0.658599  \n","3          1.00            4661         2012-11-09          -0.658599  \n","4          0.88            4661         2012-11-09          -0.658599  \n","5          0.67            4661         2012-11-09          -0.658599  \n","6          1.00            4661         2012-11-09          -0.658599  \n","7          1.00            4661         2012-11-09          -0.658599  \n","8          1.00            4661         2012-11-09          -0.658599  \n","9          1.00            4661         2012-11-09          -0.658599  "],"text/html":["\n","  <div id=\"df-b161a7d5-a820-4de8-8c69-6d4274940d75\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>posted_at</th>\n","      <th>num_comments</th>\n","      <th>score</th>\n","      <th>selftext</th>\n","      <th>subr_created_at</th>\n","      <th>subr_description</th>\n","      <th>subr_faved_by</th>\n","      <th>subr_numb_members</th>\n","      <th>subr_numb_posts</th>\n","      <th>subreddit</th>\n","      <th>title</th>\n","      <th>total_awards_received</th>\n","      <th>upvote_ratio</th>\n","      <th>user_num_posts</th>\n","      <th>user_registered_at</th>\n","      <th>user_upvote_ratio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-08-17 20:26:04</td>\n","      <td>19</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-07-06 17:01:48</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>Joe Biden's America</td>\n","      <td>0</td>\n","      <td>0.67</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-09-09 02:29:02</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>4 more years and we can erase his legacy for g...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-06-23 23:02:39</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-08-07 04:13:53</td>\n","      <td>32</td>\n","      <td>622</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>LOOK HERE, FAT</td>\n","      <td>0</td>\n","      <td>0.88</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-11-17 16:33:29</td>\n","      <td>30</td>\n","      <td>26</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>Here Is The Evidence – Crowdsourcing Evidence ...</td>\n","      <td>0</td>\n","      <td>0.67</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-08-19 17:04:16</td>\n","      <td>45</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>PJWs</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-09-04 13:38:53</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>CAN'T CUCK THE TUCK</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-09-03 20:16:52</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>Portland Antifa is Being Housed By City Democr...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-06-30 22:13:30</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>donaldtrump</td>\n","      <td>Citing State of Affairs in US, Schiff Jokes 'W...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b161a7d5-a820-4de8-8c69-6d4274940d75')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b161a7d5-a820-4de8-8c69-6d4274940d75 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b161a7d5-a820-4de8-8c69-6d4274940d75');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}],"source":["df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"lyQyR27z48nr"},"source":["## P1.1 - Text data processing (10 marks)"]},{"cell_type":"markdown","metadata":{"id":"kLUtCUL853Ln"},"source":["### P1.1.1 - Faved by as lists (3 marks)\n","\n","The column `subr_faved_by` contains an array of values (names of redditors who added the subreddit to which the current post was submitted), but unfortunately they are in text format, and you would not be able to process them properly without converting them to a suitable python type. You must convert these string values to Python lists, going from\n","\n","```python\n","'[\"user1\", \"user2\" ... ]'\n","```\n","\n","to\n","\n","```python\n","[\"user1\", \"user2\" ... ]\n","```\n","\n","**What to implement:** Implement a function `transform_faves(df)` which takes as input the original dataframe and returns the same dataframe, but with one additional column called `subr_faved_by_as_list`, where you have the same information as in `subr_faved_by`, but as a python list instead of a string."]},{"cell_type":"code","source":["def transform_faves(df):\n","    \"\"\"\n","    Function creates a new column \"subr_faved_by_as_list\" from the column \"subr_faved_by\" and inserts it into an original dataframe\n","\n","    Parameters\n","    ----------\n","    df : Pandas DATAFRAME\n","        dataframe that includes, but not limited to a column \"subr_faved_by\" in a text format\n","\n","    Returns\n","    -------\n","    df : Pandas DATAFRAME\n","        original dataframe with an inserted column \"subr_faved_by_as_list\" in a list format\n","\n","    \"\"\"\n","    df[\"subr_faved_by_as_list\"] = \"\" # create a new column \"subr_faved_by_as_list\" in an original dataframe\n","    for n in range(len(df.subr_faved_by)): # iterate through \"subr_faved_by\" column\n","      x = re.sub(\",|'\", \"\", df.subr_faved_by[n]) # replace \",\" and \"'\" with \"\" in a captured value in a \"subr_faved_by\" column and assign a renewed string to a variable x\n","      x = x[1:-1] # remove \"[\" from the start and \"]\" from the end of the string x\n","      mylist = list(x.split(\" \")) # reformat string x into a list format and assign it to mylist\n","      df.subr_faved_by_as_list[n] = mylist # let list mylist to be a value in a captured column \"subr_faved_by_as_list\"\n","    return df\n","df = transform_faves(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBVxdKb0G9nf","executionInfo":{"status":"ok","timestamp":1653039738585,"user_tz":-60,"elapsed":19208,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"840340fb-0fe8-459f-9e09-734f9190fb8f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}]},{"cell_type":"markdown","metadata":{"id":"yhZ3u5aS3rrm"},"source":["### P1.1.2 - Merge titles and text bodies (4 marks)\n","\n","All Reddit posts need to have a title, but a text body is optional. However, we want to be able to access all free text information for each post without having to look at two columns every time.\n","\n","**What to implement**: A function `concat(df)` that will take as input the original dataframe and will return it with an additional column called `full_text`, which will concatenate `title` and `selftext` columns, but with the following restrictions:\n","\n","- 1) Wrap the title between `<title>` and `</title>` tags.\n","- 2) Add a new line (`\\n`) between title and selftext, but only in cases where you have both values (see instruction 4).\n","- 3) Wrap the selftext between `<selftext>` and `</selftext>`.\n","- 4) You **must not** include the tags in points (1) or (3) if the values for these columns is missing. We will consider a missing value either an empty value (empty string) or a string of only one character (e.g., an emoji). Also, the value of a `full_text` column must not end in the new line character."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RsmY-JB39N2m","executionInfo":{"status":"ok","timestamp":1653039750664,"user_tz":-60,"elapsed":12097,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d435d2f-026a-46ca-97ff-b0fce3c3dbd3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}],"source":["def concat(df):\n","    \"\"\"\n","    Function creates an additional column called full_text, which concatenates title and selftext columns and inserts it into an original dataframe\n","\n","    Parameters\n","    ----------\n","    df : Pandas DATAFRAME\n","        dataframe that includes, but not limited to columns \"title\" and \"selftext\"\n","\n","    Returns\n","    -------\n","    df : Pandas DATAFRAME\n","        original dataframe with included \"full_text\" column\n","\n","    \"\"\"\n","    count = 0 # count for a while loop\n","    df[\"full_text\"] = \"\" # creates another column in the given dataframe\n","    mylist = list() # creates a list mylist\n","    while count < len(df.title): # iterate through \"title\" column\n","      toAdd = \"\" # create an empty string toAdd\n","      title = df.title[count] # let a variable title be the captured value in a column \"title\"\n","      selftext = df.selftext[count] # let a variable selftext be the captured value in a column \"selftext\"\n","      if len(title) > 1: # check whether a variable title is either an empty value (empty string) or a string of only one character (e.g., an emoji)\n","        toAdd = \"<title>\" + title + \"</title>\" # wrap the variable title with title tags\n","        if len(selftext) > 1: # check whether a variable selftext is either an empty value (empty string) or a string of only one character (e.g., an emoji)\n","          toAdd = toAdd + \"\\n\" + \"<selftext>\" + selftext + \"</selftext>\" # wrap the variable selftext with selftext tags and add \"\\n\" to separate it from the title\n","      else: # if a variable title is either an empty value (empty string) or a string of only one character (e.g., an emoji)\n","        if len(selftext) > 1: # check whether a variable selftext is either an empty value (empty string) or a string of only one character (e.g., an emoji)\n","          toAdd = \"<selftext>\" + selftext + \"</selftext>\" # wrap the variable selftext with selftext tags\n","      df[\"full_text\"][count] = toAdd # add a resulting string to \"full_text\" column\n","      count += 1 # add 1 to a count variable to continue an iteration process\n","    return df\n","df = concat(df)"]},{"cell_type":"markdown","metadata":{"id":"ADWvbAIe4TVd"},"source":["### P1.1.3 - Enrich posts (3 marks)\n","\n","We would like to augment our text data with linguistic information. To this end, we will _tokenize_, apply _part-of-speech tagging_, and then we will _lower case_ all the posts.\n","\n","**What to implement**: A function `enrich_posts(df)` that will take as input the original dataframe and will return it with **two** additional columns: `enriched_title` and `enriched_selftext`. These columns will contain tokenized, pos-tagged and lower cased versions of the original text. **You must implement them in this order**, because the pos tagger uses casing information."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_nDnaSwI46T_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653039816477,"user_tz":-60,"elapsed":65831,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"61d5a729-5583-4f15-bf6a-6234afd2cd32"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}],"source":["def enrich_posts(df):\n","    \"\"\"\n","    Function adds two additional columns \"enriched_title\" and \"enriched_selftext\" to an original dataframe, which contain tokenized, pos-tagged and lower cased versions of the original \"title\" and \"selftext\" columns.\n","\n","    Parameters\n","    ----------\n","    df : Pandas DATAFRAME\n","        dataframe that includes, but not limited to columns \"title\" and \"selftext\"\n","\n","    Returns\n","    -------\n","    df : Pandas DATAFRAME\n","        original dataframe with included \"enriched_title\" and \"enriched_selftext\" columns\n","\n","    \"\"\"\n","    count = 0 # count for a while loop\n","    mylist = list() # creates a list mylist\n","    mytuple = tuple() # creates a tuple mytuple\n","    df[\"enriched_title\"] = \"\" # creates another column in the given dataframe\n","    df[\"enriched_selftext\"] = \"\" # creates another column in the given dataframe\n","    while count < len(df.title): # iterate through \"title\" column\n","      tokenized = word_tokenize(df.title[count]) # let a variable tokenized be a tokenized version of a captured value in a \"title\" column\n","      tagged = pos_tag(tokenized) # let a variable tagged be a tagged version of a variable tokenized\n","      for i in range(len(tagged)): # iterate through a variable tagged\n","        mytuple = (tagged[i][0].lower(), tagged[i][1]) # let a variable mytuple contain a lower cased string of the variable tagged and its tag \n","        mylist.append(mytuple) # add a tuple mytuple to the list mylist\n","      df[\"enriched_title\"][count] = mylist # add a resulting list to \"enriched_title\" column\n","      mylist = [] # empty a list mylist\n","      count += 1 # add 1 to a count variable to continue an iteration process\n","    mylist = [] # empty a list mylist\n","    count = 0 # let a variable count be 0 to be able to start a while loop from the beginning \n","    while count < len(df.selftext): # iterate through \"selftext\" column\n","      tokenized = word_tokenize(df.selftext[count]) # let a variable tokenized be a tokenized version of a captured value in a \"selftext\" column\n","      tagged = pos_tag(tokenized) # let a variable tagged be a tagged version of a variable tokenized\n","      for i in range(len(tagged)): # iterate through a variable tagged\n","        mytuple = (tagged[i][0].lower(), tagged[i][1]) # let a variable mytuple contain a lower cased string of the variable tagged and its tag\n","        mylist.append(mytuple) # add a tuple mytuple to the list mylist\n","      df[\"enriched_selftext\"][count] = mylist  # add a resulting list to \"enriched_selftext\" column\n","      mylist = [] # empty a list mylist\n","      count +=1 # add 1 to a count variable to continue an iteration process\n","    return df\n","df = enrich_posts(df)"]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":525},"id":"EgbxVG-z1zY8","executionInfo":{"status":"ok","timestamp":1653039816478,"user_tz":-60,"elapsed":27,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"579c468d-97aa-4630-bfc3-5bea7bbc43aa"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       author            posted_at  num_comments  score selftext  \\\n","0  -Howitzer-  2020-08-17 20:26:04            19      1            \n","1  -Howitzer-  2020-07-06 17:01:48             1      3            \n","2  -Howitzer-  2020-09-09 02:29:02             3      1            \n","3  -Howitzer-  2020-06-23 23:02:39             2      1            \n","4  -Howitzer-  2020-08-07 04:13:53            32    622            \n","\n","  subr_created_at              subr_description  \\\n","0      2009-04-29  Subreddit about Donald Trump   \n","1      2009-04-29  Subreddit about Donald Trump   \n","2      2009-04-29  Subreddit about Donald Trump   \n","3      2009-04-29  Subreddit about Donald Trump   \n","4      2009-04-29  Subreddit about Donald Trump   \n","\n","                                       subr_faved_by  subr_numb_members  \\\n","0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n","\n","   subr_numb_posts  ...                                              title  \\\n","0           796986  ...  BREAKING: Trump to begin hiding in mailboxes t...   \n","1           796986  ...                                Joe Biden's America   \n","2           796986  ...  4 more years and we can erase his legacy for g...   \n","3           796986  ...  Revelation 9:6 [Transhumanism: The New Religio...   \n","4           796986  ...                                     LOOK HERE, FAT   \n","\n","  total_awards_received  upvote_ratio  user_num_posts  user_registered_at  \\\n","0                     0          1.00            4661          2012-11-09   \n","1                     0          0.67            4661          2012-11-09   \n","2                     0          1.00            4661          2012-11-09   \n","3                     0          1.00            4661          2012-11-09   \n","4                     0          0.88            4661          2012-11-09   \n","\n","  user_upvote_ratio                              subr_faved_by_as_list  \\\n","0         -0.658599  [vergil_never_cry, Jelegend, pianoyeah, salomo...   \n","1         -0.658599  [vergil_never_cry, Jelegend, pianoyeah, salomo...   \n","2         -0.658599  [vergil_never_cry, Jelegend, pianoyeah, salomo...   \n","3         -0.658599  [vergil_never_cry, Jelegend, pianoyeah, salomo...   \n","4         -0.658599  [vergil_never_cry, Jelegend, pianoyeah, salomo...   \n","\n","                                           full_text  \\\n","0  <title>BREAKING: Trump to begin hiding in mail...   \n","1                 <title>Joe Biden's America</title>   \n","2  <title>4 more years and we can erase his legac...   \n","3  <title>Revelation 9:6 [Transhumanism: The New ...   \n","4                      <title>LOOK HERE, FAT</title>   \n","\n","                                      enriched_title enriched_selftext  \n","0  [(breaking, NN), (:, :), (trump, NN), (to, TO)...                []  \n","1  [(joe, NNP), (biden, NNP), ('s, POS), (america...                []  \n","2  [(4, CD), (more, JJR), (years, NNS), (and, CC)...                []  \n","3  [(revelation, NN), (9:6, CD), ([, JJ), (transh...                []  \n","4     [(look, NNP), (here, NNP), (,, ,), (fat, NNP)]                []  \n","\n","[5 rows x 21 columns]"],"text/html":["\n","  <div id=\"df-4d0555c6-792f-4d03-917c-6d87025569a1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>posted_at</th>\n","      <th>num_comments</th>\n","      <th>score</th>\n","      <th>selftext</th>\n","      <th>subr_created_at</th>\n","      <th>subr_description</th>\n","      <th>subr_faved_by</th>\n","      <th>subr_numb_members</th>\n","      <th>subr_numb_posts</th>\n","      <th>...</th>\n","      <th>title</th>\n","      <th>total_awards_received</th>\n","      <th>upvote_ratio</th>\n","      <th>user_num_posts</th>\n","      <th>user_registered_at</th>\n","      <th>user_upvote_ratio</th>\n","      <th>subr_faved_by_as_list</th>\n","      <th>full_text</th>\n","      <th>enriched_title</th>\n","      <th>enriched_selftext</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-08-17 20:26:04</td>\n","      <td>19</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>...</td>\n","      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","      <td>[vergil_never_cry, Jelegend, pianoyeah, salomo...</td>\n","      <td>&lt;title&gt;BREAKING: Trump to begin hiding in mail...</td>\n","      <td>[(breaking, NN), (:, :), (trump, NN), (to, TO)...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-07-06 17:01:48</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>...</td>\n","      <td>Joe Biden's America</td>\n","      <td>0</td>\n","      <td>0.67</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","      <td>[vergil_never_cry, Jelegend, pianoyeah, salomo...</td>\n","      <td>&lt;title&gt;Joe Biden's America&lt;/title&gt;</td>\n","      <td>[(joe, NNP), (biden, NNP), ('s, POS), (america...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-09-09 02:29:02</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>...</td>\n","      <td>4 more years and we can erase his legacy for g...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","      <td>[vergil_never_cry, Jelegend, pianoyeah, salomo...</td>\n","      <td>&lt;title&gt;4 more years and we can erase his legac...</td>\n","      <td>[(4, CD), (more, JJR), (years, NNS), (and, CC)...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-06-23 23:02:39</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>...</td>\n","      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n","      <td>0</td>\n","      <td>1.00</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","      <td>[vergil_never_cry, Jelegend, pianoyeah, salomo...</td>\n","      <td>&lt;title&gt;Revelation 9:6 [Transhumanism: The New ...</td>\n","      <td>[(revelation, NN), (9:6, CD), ([, JJ), (transh...</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-Howitzer-</td>\n","      <td>2020-08-07 04:13:53</td>\n","      <td>32</td>\n","      <td>622</td>\n","      <td></td>\n","      <td>2009-04-29</td>\n","      <td>Subreddit about Donald Trump</td>\n","      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n","      <td>30053</td>\n","      <td>796986</td>\n","      <td>...</td>\n","      <td>LOOK HERE, FAT</td>\n","      <td>0</td>\n","      <td>0.88</td>\n","      <td>4661</td>\n","      <td>2012-11-09</td>\n","      <td>-0.658599</td>\n","      <td>[vergil_never_cry, Jelegend, pianoyeah, salomo...</td>\n","      <td>&lt;title&gt;LOOK HERE, FAT&lt;/title&gt;</td>\n","      <td>[(look, NNP), (here, NNP), (,, ,), (fat, NNP)]</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d0555c6-792f-4d03-917c-6d87025569a1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4d0555c6-792f-4d03-917c-6d87025569a1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4d0555c6-792f-4d03-917c-6d87025569a1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"I8E010UbQyML"},"source":["## P1.2 - Answering questions with pandas (12 marks)\n","\n","In this question, your task is to use pandas to answer questions about the data."]},{"cell_type":"markdown","metadata":{"id":"7ZmG2VIYQ93I"},"source":["### P1.2.1 - Users with best scores (3 marks)\n","\n","- Find the users with the highest aggregate scores (over all their posts) for the whole dataset. You should restrict your results to only those whose aggregated score is above 10,000 points, in descending order. Your code should generate a dictionary of the form `{author:aggregated_scores ... }`."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RhW8Rr6QSXDj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653039816789,"user_tz":-60,"elapsed":319,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"218e1aa4-387a-48dd-b279-bfd6f71bf151"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'DaFunkJunkie': 250375, 'None': 218846, 'SUPERGUESSOUS': 211611, 'jigsawmap': 210784, 'chrisdh79': 143538, 'hildebrand_rarity': 122464, 'iSlingShlong': 118595, 'hilltopye': 81245, 'tefunka': 79560, 'OldFashionedJizz': 64398, 'JLBesq1981': 58235, 'rspix000': 57107, 'Wagamaga': 47989, 'stem12345679': 47455, 'TheJeck': 26058, 'TheGamerDanYT': 25357, 'TrumpSharted': 21154, 'NotsoPG': 18518, 'SonictheManhog': 18116, 'BlanketMage': 13677, 'NewAltWhoThis': 12771, 'kevinmrr': 11900, 'Dajakesta0624': 11613, 'apocalypticalley': 10382}\n"]}],"source":["x = {} # let x be an empty dictionary\n","count = 0 # count for a while loop\n","while count < len(df.author): # iterate through \"author\" column\n","  if count == 0: # if it's a first iteration\n","    author = df.author[count] # let a variable author be a captured value in a \"author\" column\n","    score = df.score[count] # let a variable score be a captured value in a \"score\" column\n","    count += 1 # add 1 to a variable count to continue an iteration process\n","  else: # if it's not a first iteration\n","    if author == df.author[count]: # if a captured value in a \"author\" column matches an author variable\n","      score += df.score[count] # add its score to a score variable\n","    else: # if a captured value in a \"author\" column doesn't match an author variable\n","      if score > 10000: # if a score variable is larger than 10,000\n","        x[author] = score # add its key and value to a dictionary x\n","      author = df.author[count] # let a variable author be a captured value in a \"author\" column\n","      score = df.score[count] # let a variable score be a captured value in a \"score\" column\n","    count += 1 # add 1 to a variable count to continue an iteration process\n","count = 0 # let a variable count be 0 to be able to start a while loop from the beginning \n","x_sorted_keys = sorted(x, key=x.get, reverse=True) # let a variable x_sorted_keys be sorted by keys of a variable x\n","x_sorted = {} # create an empty dictionary x_sorted\n","while count < len(x_sorted_keys): # iterate through a variable x_sorted_keys\n","  x_sorted[x_sorted_keys[count]] = x[x_sorted_keys[count]] # let an author's name be a key and an author's score be a value and add it to the dictionary x_sorted\n","  count += 1 # add 1 to a variable count to continue an iteration process\n","print(x_sorted) # print a dictionary of the users with the highest aggregate scores (over all their posts) for the whole dataset, whose aggregated score is above 10,000 points, in descending order."]},{"cell_type":"markdown","metadata":{"id":"woOFrPFQT5cZ"},"source":["### P1.2.2 - Awarded posts (3 marks)\n","\n","Find the number of posts that have received at least one award. Your query should return only one value."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0fVuaWmmUGVW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653039817091,"user_tz":-60,"elapsed":304,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"19d84c8a-2c80-4a1b-d5f8-fd0d1cb47b14"},"outputs":[{"output_type":"stream","name":"stdout","text":["119\n"]}],"source":["count = 0 # count for a while loop\n","result = 0 # let a variable result be 0\n","while count < len(df.total_awards_received): # iterate through a \"total_awards_received\" column\n","  if df.total_awards_received[count] > 0: # if a captured value in a \"total_awards_received\" column is greater than 0\n","    result += 1 # add 1 to a variable result\n","  count += 1 # add 1 to a variable count to continue an iteration process\n","print(result) # print the number of posts that have received at least one award"]},{"cell_type":"markdown","metadata":{"id":"uVj1WikSUPjO"},"source":["### P1.2.3 Find Covid (3 marks)\n","\n","Find the name and description of all subreddits where the name starts with `Covid` or `Corona` and the description contains `covid` or `Covid` anywhere. Your code should generate a dictionary of the form#\n","\n","```python\n","  {'Coronavirus':'Place to discuss all things COVID-related',\n","  ...\n","  }\n","```"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"w6fIWO8BUhu3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653039817748,"user_tz":-60,"elapsed":659,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"9d8c0543-c788-4d04-cb47-bdad3f0b7624"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'CoronavirusCA': 'Tracking the Coronavirus/Covid-19 outbreak in California'}\n"]}],"source":["count = 0 # count for a while loop\n","regexSub = re.compile(\"^Covid|^Corona\") # let a variable regexSub be a regex for subreddits where the name starts with Covid or Corona\n","regexDesc = re.compile(\"Covid|covid\") # let a variable regexDesc be a regex for descriptions that contain covid or Covid anywhere\n","d = {} # let d be an empty dictionary\n","while count < len(df.subreddit): # iterate through a \"subreddit\" column\n","  if re.search(regexSub, df.subreddit[count]) and re.search(regexDesc, df.subr_description[count]): # if a captured value in \"subreddit\" column matches regexSub and regexDesc regexs\n","    d[df.subreddit[count]] = df.subr_description[count] # add it to the dictionary d\n","  count += 1 # add 1 to a variable count to continue an iteration process\n","print(d) # print a resulting dictionary"]},{"cell_type":"markdown","metadata":{"id":"ToPttp2-fsXG"},"source":["### P1.2.4 - Redditors that favorite the most\n","\n","Find the users that have favorited the largest number of subreddits. You must produce a pandas dataframe with **two** columns, with the following format:\n","\n","```python\n","     redditor\t    numb_favs\n","0\tuser1           7\n","1\tuser2           6\n","2\tuser3\t       5\n","3\tuser4           4\n","...\n","```\n","\n","where the first column is a Redditor username and the second column is the number of distinct subreddits he/she has favorited."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"LbFeie3jip44","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653039818539,"user_tz":-60,"elapsed":794,"user":{"displayName":"dmarych dmarych","userId":"06500807467530056041"}},"outputId":"9ede1002-f217-4930-83c7-48da0f7c25f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["                  redditor  numb_favs\n","0           magnusthered15          7\n","1                 ry_ta506          6\n","2                KarmaFury          6\n","3     FriendlyVegetable420          6\n","4              OmniusQubus          6\n","...                    ...        ...\n","1594   certifiedloverboy69          1\n","1595            diveonfire          1\n","1596         mouthofreason          1\n","1597               Alexify          1\n","1598        lilstinky[***]          1\n","\n","[1599 rows x 2 columns]\n"]}],"source":["count = 0 # count for a while loop\n","countInt = 0 # internal count for a while loop\n","s = list() # let s be a list\n","redd = list() # let redd be a list\n","users = list() # lets users be a list\n","d = {} # let d be a dictionary\n","while count < len(df.subreddit): # iterate through a \"subreddit\" column\n","  if df.subreddit[count] not in s: # if a captured value in a \"subreddit\" column is not in the list s\n","    s.append(df.subreddit[count]) # add a captured value in a \"subreddit\" column to a list s\n","  count += 1 # add 1 to a variable count to continue an iteration process\n","count = 0 # let a variable count be 0 to be able to start a while loop from the beginning\n","while count < len(df.subr_faved_by_as_list): # iterate through a \"subr_faved_by_as_list\" column\n","  if count == 0: # if it's a first iteration\n","    redd.append(df.subr_faved_by_as_list[count]) # add a captured value in a \"subr_faved_by_as_list\" column to a list redd\n","    countInt += 1 # add 1 to a variable countInt to continue an iteration process\n","  else: # if it's not a first iteration\n","    if df.subreddit[count] == s[countInt]: # if a captured value in a \"subreddit\" column equals a captured value in a list s\n","      redd.append(df.subr_faved_by_as_list[count]) # add a captured value in a \"subr_faved_by_as_list\" column to a list redd\n","      countInt += 1 # add 1 to a variable countInt to continue an iteration process\n","  count += 1 # add 1 to a variable count to continue an iteration process\n","  if countInt == len(s): # if a variable countInt equals a length of a list s\n","    break # stop the iteration process\n","for n in range(len(s)): # iterate through list s\n","  for i in range(len(redd[n])): # iterate through a captured list within a list redd\n","    users.append(redd[n][i]) # add a captured value within a list of a list redd to a list users\n","for n in users: # iterate through a list users\n","  d[n] = users.count(n) # let the captured value from the list users be a key in a dictionary d and its number of occurrences be its value \n","sorted_keys = sorted(d, key=d.get, reverse = True) # let sorted_keys be a sorted list of redditors by their number of favourites in a descending order\n","d1 = {} # let d1 be an empty dictionary\n","for n in sorted_keys: # iterate through a list sorted_keys\n","    d1[n] = d[n] # let the captured value from the list sorted_keys be a key in a dictionary d1 and its number of favourites be its value\n","favUsers = pd.DataFrame(d1.items(), columns=['redditor', 'numb_favs']) # turn a dictionary d1 into a dataframe and assign labels to the columns\n","print(favUsers) # print a pandas dataframe with two columns, where the first column is a Redditor username and the second column is the number of distinct subreddits he/she has favorited"]},{"cell_type":"markdown","metadata":{"id":"RsAF9jpblJLp"},"source":["## P1.3 Ethics (8 marks)\n","\n","**(updated on 16/03/2022)**\n","\n","Imagine you are **the head of a data mining company that needs to use** the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). **Some information about the project and the team:**\n","\n"," - Your client is a political party concerned about misinformation.\n"," - The project requires mining Facebook, Reddit and Instagram data.\n"," - The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n","\n","Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UK’s Data Ethics Framework. \n","\n","Your answer should address the following:\n","\n"," - Identify the action **in which your project is the weakest**.\n"," - Then, justify your choice by critically analyzing the three key principles **for that action** outlined in the Framework, namely transparency, accountability and fairness.\n"," - Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n","\n","Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit.**"]},{"cell_type":"markdown","metadata":{"id":"8YJQSO8Amuea"},"source":["---\n","\n","COVID-related content is a very touchy subject to some, if not most people. Depending on the nationality, origin and/or political beliefs different people might react differently to the same content(Yoo, J., Dutra, S.V.O., Fanfan, D. et al., 2020). Luckily, our team consists of people with different backgrounds and ages. Diversity plays a big part in this project since different people with such different ethical backgrounds can analyze and sort information differently(Sylvia Ann Hewlett, Melinda Marshall, and Laura Sherbin, 2013). It helps a lot in identifying misuse of information or straight-up hate. The biggest problem, in my opinion, in identifying such content as potentially harmful is that it might offend some people even if an article or a post is written for educational reasons. In the modern world, people, in their masses, are getting offended more and more each coming day, even if statements are backed by the research and were not intended to insult or hurt, but rather to teach and educate. For me, this is where the line should be drawn. One just can’t please everyone. We have a whole world at our fingertips to search, analyze and think about the information we gather from different sources. Nobody is forcing anyone to read or react to everything they see. Nobody has cancelled freedom of expression, as long as there is no hate speech or a direct intent to repress. Transparency is key with information of this kind, since it may save lives, which is the most important thing we have. However, every content on this topic needs to be carefully reviewed, as, even if it’s properly cited, there is always a chance for misuse of information. Social media is a great platform for the freedom of speech, however, there has been little to no accountability, among regular people, to one's words (Harry Bruinius, 2018). Anyone can, basically, write anything they wish, whether it’s true or false, right or wrong, or offensive or harmless. We should always remember that we do not know who is sitting behind the device, from which those messages are sent. This is where Data Science comes into clutch. With the power of machines, we can sort out information faster than it reaches people, who might not be very pleased with the information provided. However, we should be fair to each publication, regardless of its origin. The only things which should matter are its intent, wording, conclusion and proper use of information. A particular solution to the misuse of information could be an implementation of responsibilities by the government. Although superficially existent, the government’s intervention in particular cases of the use of information would be of great help to society as a whole, since people need to only know the truth about the topics of such seriousness. Everyone must feel obligated to provide well-researched information, as well as a valid interpretation of it, in order to not mislead others, as it can cause severe damage to the readers as well as to lots of people around them. Such touchy topics like COVID, need to be highly monitored at all times, as it's in the government’s interest to convey reliable and truthful information to its citizens, permanent and temporary residents.\n","\n","References\n","\n","Yoo, J., Dutra, S.V.O., Fanfan, D. et al. Comparative analysis of COVID-19 guidelines from six countries: a qualitative study on the US, China, South Korea, the UK, Brazil, and Haiti. BMC Public Health 20, 1853 (2020). https://doi.org/10.1186/s12889-020-09924-7\n","\n","Sylvia Ann Hewlett, Melinda Marshall, and Laura Sherbin. How Diversity Can Drive Innovation. Harvard Business Review (2013). https://hbr.org/2013/12/how-diversity-can-drive-innovation\n","\n","Harry Bruinius. Are you what you post? Social media and the accountability debate. The Christian Science Monitor (2018). https://www.csmonitor.com/USA/Politics/2018/0809/Are-you-what-you-post-Social-media-and-the-accountability-debate\n","\n","https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/923108/Data_Ethics_Framework_2020.pdf\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"P1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}